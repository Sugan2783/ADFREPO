{
	"name": "dataflow_ASSERT_TRF",
	"properties": {
		"folder": {
			"name": "ADF_TRAINING_DATAFLOW"
		},
		"type": "MappingDataFlow",
		"typeProperties": {
			"sources": [
				{
					"dataset": {
						"referenceName": "ADF_TRAIN_SQLDB",
						"type": "DatasetReference"
					},
					"name": "source1"
				},
				{
					"dataset": {
						"referenceName": "ADF_TRAIN_SQLDB",
						"type": "DatasetReference"
					},
					"name": "source2"
				}
			],
			"sinks": [
				{
					"dataset": {
						"referenceName": "ADLS_PARAM",
						"type": "DatasetReference"
					},
					"name": "sink1"
				},
				{
					"dataset": {
						"referenceName": "ADLS_PARAM",
						"type": "DatasetReference"
					},
					"name": "sink2"
				},
				{
					"dataset": {
						"referenceName": "ADLS_PARAM",
						"type": "DatasetReference"
					},
					"name": "sink3"
				},
				{
					"dataset": {
						"referenceName": "ADLS_PARAM",
						"type": "DatasetReference"
					},
					"name": "sink4"
				}
			],
			"transformations": [
				{
					"name": "assert1"
				},
				{
					"name": "derivedColumn1"
				},
				{
					"name": "split1"
				}
			],
			"scriptLines": [
				"source(output(",
				"          Claim_ID as integer,",
				"          claimant_name as string,",
				"          claim_amount as integer,",
				"          claim_Date as string,",
				"          location_id as integer",
				"     ),",
				"     allowSchemaDrift: true,",
				"     validateSchema: false,",
				"     isolationLevel: 'READ_UNCOMMITTED',",
				"     format: 'table') ~> source1",
				"source(output(",
				"          location_id as integer,",
				"          location_name as string",
				"     ),",
				"     allowSchemaDrift: true,",
				"     validateSchema: false,",
				"     isolationLevel: 'READ_UNCOMMITTED',",
				"     format: 'table') ~> source2",
				"source1, source2 assert(expectUnique(Claim_ID, false, 'uniqueid'),",
				"     expectTrue(! isNull(toDate((claim_Date), 'yyyy-MM-dd')), false, 'dateformat'),",
				"     expectExists(source1@location_id == source2@location_id, false, 'locationexists')) ~> assert1",
				"assert1 derive(ISERROR = isError(),",
				"          duplicateid = hasError('uniqueid'),",
				"          dateformat = hasError('dateformat'),",
				"          locexists = hasError('locationexists')) ~> derivedColumn1",
				"derivedColumn1 split(duplicateid == true(),",
				"     locexists == true(),",
				"     dateformat == true(),",
				"     disjoint: false) ~> split1@(primarykeyerror, Locationexists, dateformat, Goodrows)",
				"split1@primarykeyerror sink(allowSchemaDrift: true,",
				"     validateSchema: false,",
				"     partitionFileNames:['primaryerr.csv'],",
				"     umask: 0022,",
				"     preCommands: [],",
				"     postCommands: [],",
				"     skipDuplicateMapInputs: true,",
				"     skipDuplicateMapOutputs: true,",
				"     partitionBy('hash', 1)) ~> sink1",
				"split1@Locationexists sink(allowSchemaDrift: true,",
				"     validateSchema: false,",
				"     partitionFileNames:['locexists.csv'],",
				"     umask: 0022,",
				"     preCommands: [],",
				"     postCommands: [],",
				"     skipDuplicateMapInputs: true,",
				"     skipDuplicateMapOutputs: true,",
				"     partitionBy('hash', 1)) ~> sink2",
				"split1@dateformat sink(allowSchemaDrift: true,",
				"     validateSchema: false,",
				"     partitionFileNames:['dateformat.csv'],",
				"     umask: 0022,",
				"     preCommands: [],",
				"     postCommands: [],",
				"     skipDuplicateMapInputs: true,",
				"     skipDuplicateMapOutputs: true,",
				"     partitionBy('hash', 1)) ~> sink3",
				"split1@Goodrows sink(allowSchemaDrift: true,",
				"     validateSchema: false,",
				"     partitionFileNames:['goodrows.csv'],",
				"     umask: 0022,",
				"     preCommands: [],",
				"     postCommands: [],",
				"     skipDuplicateMapInputs: true,",
				"     skipDuplicateMapOutputs: true,",
				"     partitionBy('hash', 1)) ~> sink4"
			]
		}
	}
}